{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "from datasets import Dataset, Features, Sequence, ClassLabel, Value\n",
    "import numpy as np\n",
    "from seqeval.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from huggingface_hub import login\n",
    "\n",
    "login(token='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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read TSV files and parse sentences and labels\n",
    "def read_tsv_file(file_path):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        tokens = []\n",
    "        ner_tags = []\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                token, tag = line.strip().split('\\t')\n",
    "                tokens.append(token)\n",
    "                ner_tags.append(tag)\n",
    "            else:\n",
    "                if tokens:\n",
    "                    sentences.append(tokens)\n",
    "                    labels.append(ner_tags)\n",
    "                    tokens = []\n",
    "                    ner_tags = []\n",
    "        if tokens:\n",
    "            sentences.append(tokens)\n",
    "            labels.append(ner_tags)\n",
    "    return sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences, train_labels = read_tsv_file('../datasets/ner/BC2GM/train.tsv')\n",
    "dev_sentences, dev_labels = read_tsv_file('../datasets/ner/BC2GM/dev.tsv')\n",
    "test_sentences, test_labels = read_tsv_file('../datasets/ner/BC2GM/test.tsv')\n",
    "label_list = ['B', 'I', 'O']\n",
    "id2label = {i:label for i,label in enumerate(label_list)}\n",
    "label2id = {label:i for i,label in enumerate(label_list)}\n",
    "\n",
    "# Create datasets using the datasets package\n",
    "features = Features({\n",
    "    'tokens': Sequence(Value('string')),\n",
    "    'ner_tags': Sequence(\n",
    "        ClassLabel(names=label_list)\n",
    "    )\n",
    "})\n",
    "\n",
    "train_data = {'tokens': train_sentences, 'ner_tags': train_labels}\n",
    "dev_data = {'tokens': dev_sentences, 'ner_tags': dev_labels}\n",
    "test_data = {'tokens': test_sentences, 'ner_tags': test_labels}\n",
    "\n",
    "train_dataset = Dataset.from_dict(train_data, features=features)\n",
    "dev_dataset = Dataset.from_dict(dev_data, features=features)\n",
    "test_dataset = Dataset.from_dict(test_data, features=features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased', use_fast=True) # use faster tokenizer\n",
    "\n",
    "# Tokenize and align labels\n",
    "def tokenize_and_align_labels(batch):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        batch['tokens'],\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        is_split_into_words=True,\n",
    "    )\n",
    "    labels = []\n",
    "    for i, label in enumerate(batch['ner_tags']):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs['labels'] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the tokenization to the datasets\n",
    "train_dataset = train_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "dev_dataset = dev_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the format for PyTorch tensors\n",
    "columns = ['input_ids', 'attention_mask', 'labels']\n",
    "train_dataset.set_format(type='torch', columns=columns)\n",
    "dev_dataset.set_format(type='torch', columns=columns)\n",
    "test_dataset.set_format(type='torch', columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model for token classification\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    'distilbert-base-uncased',\n",
    "    num_labels=len(label_list),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='../results/ner/BC2GM/distilbert-base-uncased',\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Define the metric function using seqeval\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_labels = [\n",
    "        [id2label[label_id] for label_id in label if label_id != -100]\n",
    "        for label in labels\n",
    "    ]\n",
    "    true_predictions = [\n",
    "        [id2label[pred_id] for (pred_id, label_id) in zip(prediction, label) if label_id != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        'precision': round(precision_score(true_labels, true_predictions), 2), \n",
    "        'recall': round(recall_score(true_labels, true_predictions), 2),\n",
    "        'f1': round(f1_score(true_labels, true_predictions), 2),\n",
    "        'accuracy': round(accuracy_score(true_labels, true_predictions), 2),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# push this model to the hub\n",
    "model.push_to_hub('distilbert-base-uncased-FT-ner-BC2GM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload the model from the hub and evaluate on the test set use trainer.predict\n",
    "model = AutoModelForTokenClassification.from_pretrained('kbulutozler/distilbert-base-uncased-FT-ner-BC2GM')\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the mdel on test set\n",
    "predictions, labels, _ = trainer.predict(test_dataset)\n",
    "compute_metrics((predictions, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
